{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.105004Z",
     "start_time": "2024-04-02T10:14:29.101328Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a658ca5455bd9de5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.212474Z",
     "start_time": "2024-04-02T10:14:29.189724Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# read data\n",
    "train_df = pd.read_csv('../MELD.Raw/train/train_sent_emo.csv')\n",
    "with open('../audio/train_data.pkl', 'rb') as f:\n",
    "    train_audio_dict = pickle.load(f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16006670-0a08-4948-b330-ae9a0fd6646c",
   "metadata": {},
   "source": [
    "# Convert StartTime and EndTime to timedelta to calculate the utterance length\n",
    "train_df['StartTime'] = pd.to_timedelta(train_df['StartTime'])\n",
    "train_df['EndTime'] = pd.to_timedelta(train_df['EndTime'])\n",
    "train_df['Utterance_Length'] = (train_df['EndTime'] - train_df['StartTime']).dt.total_seconds()\n",
    "\n",
    "# Calculate the average utterance length by Sentiment\n",
    "avg_utterance_length = train_df.groupby('Sentiment')['Utterance_Length'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['skyblue', 'lightgreen', 'salmon', 'lightcoral', 'gold']  # Different colors for each sentiment\n",
    "plt.bar(avg_utterance_length['Sentiment'], avg_utterance_length['Utterance_Length'], color=colors)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Average Utterance Length (seconds)')\n",
    "plt.title('Average Utterance Length by Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "65debc96efc7e119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.247347Z",
     "start_time": "2024-04-02T10:14:29.242444Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# set encoders\n",
    "emotion_encoder = LabelEncoder()\n",
    "sentiment_encoder = LabelEncoder()\n",
    "emotion_encoder.fit(train_df['Emotion'])\n",
    "sentiment_encoder.fit(train_df['Sentiment'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba1b63308504f9d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.295414Z",
     "start_time": "2024-04-02T10:14:29.281036Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "emotion_distribution = train_df['Emotion'].value_counts(normalize=True)\n",
    "emotion_distribution"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "762679a4e8210a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.340847Z",
     "start_time": "2024-04-02T10:14:29.336349Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "sentiment_distribution = train_df['Sentiment'].value_counts(normalize=True)\n",
    "sentiment_distribution"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "162b3db59f65d383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.604985Z",
     "start_time": "2024-04-02T10:14:29.594970Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "train_df['word_count'] = train_df['Utterance'].apply(lambda x: len(x.split()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4e0ade905c2562ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.647795Z",
     "start_time": "2024-04-02T10:14:29.641540Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "train_df.groupby('Emotion')['word_count'].mean()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "782965e28c9af569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.899503Z",
     "start_time": "2024-04-02T10:14:29.893991Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "top_10_speakers = train_df.groupby('Speaker', as_index=False)['Speaker'].size()\n",
    "top_10_speakers = top_10_speakers.sort_values(by='size', ascending=False)\n",
    "top_10_speakers = top_10_speakers['Speaker'].values[0:10]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99c44e7f-197d-475d-a132-92bdb55b1f61",
   "metadata": {},
   "source": [
    "# Calculate the count of each speaker\n",
    "speaker_counts = train_df['Speaker'].value_counts()\n",
    "\n",
    "# Create a DataFrame from the speaker counts\n",
    "speaker_df = speaker_counts.reset_index()\n",
    "speaker_df.columns = ['Speaker', 'Count']\n",
    "\n",
    "# Calculate the total number of entries\n",
    "total = speaker_df['Count'].sum()\n",
    "\n",
    "# Calculate the proportion for each speaker\n",
    "speaker_df['Proportion'] = speaker_df['Count'] / total\n",
    "\n",
    "# Sort the DataFrame by the count and select the top 6\n",
    "top_6_speakers = speaker_df.sort_values(by='Count', ascending=False).head(6)\n",
    "\n",
    "print(top_6_speakers)\n",
    "print((top_6_speakers['Proportion']).sum())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "541cef1f-071c-4282-8b2d-58beab667b10",
   "metadata": {},
   "source": [
    "# Calculate the count of each speaker\n",
    "speaker_counts = train_df['Speaker'].value_counts()\n",
    "\n",
    "# Create a DataFrame from the speaker counts\n",
    "speaker_df = speaker_counts.reset_index()\n",
    "speaker_df.columns = ['Speaker', 'Count']\n",
    "\n",
    "# Calculate the total number of entries\n",
    "total = speaker_df['Count'].sum()\n",
    "\n",
    "# Calculate the proportion for each speaker\n",
    "speaker_df['Proportion'] = speaker_df['Count'] / total\n",
    "\n",
    "# Sort the DataFrame by the count\n",
    "speaker_df = speaker_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Separate the top 6 speakers\n",
    "top_6_speakers = speaker_df.head(6)\n",
    "\n",
    "# Create an 'Others' category for all other speakers\n",
    "others = pd.DataFrame(data = {\n",
    "    'Speaker': ['Others'],\n",
    "    'Count': [speaker_df['Count'][6:].sum()],\n",
    "    'Proportion': [speaker_df['Proportion'][6:].sum()]\n",
    "})\n",
    "\n",
    "# Combine the top 6 speakers with 'Others'\n",
    "plot_data = pd.concat([top_6_speakers, others])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(plot_data['Speaker'], plot_data['Proportion'], color=['blue', 'blue', 'blue', 'blue', 'blue', 'blue', 'green'])\n",
    "plt.xlabel('Speaker')\n",
    "plt.ylabel('Proportion of Total appearances')\n",
    "plt.title('Proportion of appearances on dataset by Speakers')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6044a61163474c9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.940238Z",
     "start_time": "2024-04-02T10:14:29.937525Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "top_10_speakers"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8a35172cdb57305f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.978428Z",
     "start_time": "2024-04-02T10:14:29.974947Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "train_df_top_10 = train_df[train_df['Speaker'].isin(top_10_speakers)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "44c595c056d0a4c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:30.031495Z",
     "start_time": "2024-04-02T10:14:30.026603Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "train_df_top_10.groupby('Speaker',)['word_count'].mean()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e1b4f4241ef4a3c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:30.061783Z",
     "start_time": "2024-04-02T10:14:30.057090Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "# Function to generate and display word clouds\n",
    "def generate_word_clouds(df, column):\n",
    "    # Unique categories\n",
    "    categories = df[column].unique()\n",
    "    \n",
    "    # Figure setup\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Generate a word cloud for each category\n",
    "    for i, category in enumerate(categories, 1):\n",
    "        # Aggregate text for the current category\n",
    "        text = \" \".join(df[df[column] == category]['Utterance'].tolist())\n",
    "        \n",
    "        # Generate word cloud\n",
    "        wordcloud = WordCloud(width=400, height=300, background_color='white').generate(text)\n",
    "        \n",
    "        # Plot\n",
    "        plt.subplot(1, len(categories), i)\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'{category} {column}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5fb3987b3677f9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:31.487819Z",
     "start_time": "2024-04-02T10:14:30.091603Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "generate_word_clouds(train_df, 'Sentiment')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "55f2cb194d83cbb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:37.370298Z",
     "start_time": "2024-04-02T10:14:34.969591Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "generate_word_clouds(train_df, 'Emotion')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a63315cc-8562-417a-b796-a138d6e88ed7",
   "metadata": {},
   "source": [
    "**Audio Features Descriptive Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691be548-7592-4a15-981a-fc2c54cd0f96",
   "metadata": {},
   "source": [
    "with open('../audio/test_data.pkl','rb') as file:\n",
    "   test_audio_dict = pickle.load(file)\n",
    "\n",
    "with open('../AudioFeaturesExtraction/train_data.pkl','rb') as file:\n",
    "    train_audio_dict = pickle.load(file)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "258f9f43-14b8-4bd7-992e-b12d5d4ad941",
   "metadata": {},
   "source": [
    "def extract_features(waveform, sr):\n",
    "    \"\"\"\n",
    "    Calculate various spectral features and return them in a dictionary.\n",
    "    \"\"\"\n",
    "    # Basic spectral features\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=waveform, sr=sr)[0]\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=waveform, sr=sr)[0]\n",
    "    spectral_flatness = librosa.feature.spectral_flatness(y=waveform)[0]\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=waveform, sr=sr)[0]\n",
    "    rms_energy = librosa.feature.rms(y=waveform)[0]\n",
    "    zcr = librosa.feature.zero_crossing_rate(waveform)[0]\n",
    "    mfccs = librosa.feature.mfcc(y=waveform, sr=sr)\n",
    "    chroma = librosa.feature.chroma_stft(y=waveform, sr=sr)\n",
    "\n",
    "    # Initialize the feature dictionary\n",
    "    features = {\n",
    "        'centroid_mean': np.mean(spectral_centroid),\n",
    "        'bandwidth_mean': np.mean(spectral_bandwidth),\n",
    "        'flatness_mean': np.mean(spectral_flatness),\n",
    "        'rolloff_mean': np.mean(spectral_rolloff),\n",
    "        'rms_energy_mean': np.mean(rms_energy),\n",
    "        'zcr_mean': np.mean(zcr)\n",
    "    }\n",
    "\n",
    "    # Adding MFCCs and Chroma features\n",
    "    for i in range(mfccs.shape[0]):  # Assuming MFCCs are returned with shape (n_mfcc, t)\n",
    "        features[f'mfccs_mean_{i}'] = np.mean(mfccs[i, :])\n",
    "\n",
    "    for i in range(chroma.shape[0]):  # Assuming Chroma features are returned with shape (n_chroma, t)\n",
    "        features[f'chroma_mean_{i}'] = np.mean(chroma[i, :])\n",
    "\n",
    "    return features\n",
    "\n",
    "def process_audio_data(audio_dict, sample_rate):\n",
    "    \"\"\"\n",
    "    Process each audio scene, extract features, and compile them into a single DataFrame.\n",
    "    \"\"\"\n",
    "    feature_list = []\n",
    "\n",
    "    for scene_id, data in audio_dict.items():\n",
    "        waveform = data['waveforms'][0].numpy()  # Ensure waveform is a NumPy array\n",
    "        features = extract_features(waveform, sample_rate)\n",
    "        features['scene_id'] = scene_id  # Add scene_id to the features dictionary\n",
    "        feature_list.append(features)\n",
    "    \n",
    "    # Create a DataFrame from the list of feature dictionaries\n",
    "    combined_features_df = pd.DataFrame(feature_list)\n",
    "    return combined_features_df\n",
    "\n",
    "sample_rate = 16000  # Common sample rate for high-quality audio\n",
    "test_feature_data = process_audio_data(test_audio_dict, sample_rate)\n",
    "train_feature_data = process_audio_data(train_audio_dict, sample_rate)\n",
    "\n",
    "print(test_feature_data.head())\n",
    "print(train_feature_data.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5072d80-f7c8-4378-9ce6-9c61b0d09e83",
   "metadata": {},
   "source": [
    "dev = pd.read_csv('dev_fe_16000.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "744d57d2-5324-45cb-ac4d-517538cb506d",
   "metadata": {},
   "source": [
    "def file_key_generator(file_path, labels_dict):\n",
    "    info_file = pd.read_csv(file_path)\n",
    "    # Creating file_key which is a unique identifier for each scene.\n",
    "    info_file['file_key'] = 'dia' + info_file['Dialogue_ID'].astype(str) + '_' + 'utt' + info_file[\n",
    "        'Utterance_ID'].astype(str)\n",
    "\n",
    "    info_file['label'] = info_file['Sentiment'].map(labels_dict)\n",
    "    info_file = info_file.sort_values(by='file_key')\n",
    "    return info_file\n",
    "\n",
    "labels_dict = {\n",
    "            'negative': 0,\n",
    "            'neutral': 1,\n",
    "            'positive': 2}\n",
    "file_path = '../MELD.Raw/train/train_sent_emo.csv'\n",
    "\n",
    "train_info_file = file_key_generator(file_path, labels_dict)\n",
    "train_info_file_subset = train_info_file[['file_key','label']]\n",
    "print(train_info_file_subset.head())\n",
    "\n",
    "features_data = pd.read_csv('../audio/train_fe.csv')\n",
    "audio_features= [col for col in features_data.columns if col.startswith('audio_feature_')]\n",
    "all_features = audio_features + ['file_key']\n",
    "audio_features_data = features_data[all_features]\n",
    "print(audio_features_data.head())\n",
    "train_joined_with_audio = audio_features_data.merge(train_info_file_subset, left_on = 'file_key', right_on = 'file_key')\n",
    "print(train_joined_with_audio.head())\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "140416e2-77d9-440f-a7d9-a649932acd5f",
   "metadata": {},
   "source": [
    "num_cols = ['centroid_median', 'bandwidth_median', 'flatness_median',\n",
    "       'rolloff_median', 'rms_energy_median', 'zcr_median', 'tempo',\n",
    "       'tempogram_median', 'tempogram_ratio_median', 'tonnetz_median',\n",
    "       'mfccs_median_0', 'mfccs_median_1', 'mfccs_median_2', 'mfccs_median_3',\n",
    "       'mfccs_median_4', 'mfccs_median_5', 'mfccs_median_6', 'mfccs_median_7',\n",
    "       'mfccs_median_8', 'mfccs_median_9', 'mfccs_median_10',\n",
    "       'mfccs_median_11', 'mfccs_median_12', 'mfccs_median_13',\n",
    "       'mfccs_median_14', 'mfccs_median_15', 'mfccs_median_16',\n",
    "       'mfccs_median_17', 'mfccs_median_18', 'mfccs_median_19',\n",
    "       'chroma_median_0', 'chroma_median_1', 'chroma_median_2',\n",
    "       'chroma_median_3', 'chroma_median_4', 'chroma_median_5',\n",
    "       'chroma_median_6', 'chroma_median_7', 'chroma_median_8',\n",
    "       'chroma_median_9', 'chroma_median_10', 'chroma_median_11']\n",
    "df_cor = train_joined_with_audio[num_cols].corr(method='spearman')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1509c473-2d0e-4778-8689-76db589521a1",
   "metadata": {},
   "source": [
    "train_joined_with_audio['label'].value_counts(normalize=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63341daa-4097-436b-815a-618204c5de18",
   "metadata": {},
   "source": [
    "train_joined_with_audio['label'].value_counts(normalize=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f373aac-cf65-4b5b-bc57-7896dcc165b7",
   "metadata": {},
   "source": [
    "features = dev_joined_with_audio.columns[:-3]\n",
    "\n",
    "# Set the aesthetics for the plots\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Create a figure to hold the plots\n",
    "fig, axes = plt.subplots(10, 4, figsize=(20, 50))  # Adjust the layout size as necessary\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    # Plot each feature by label in the same subplot\n",
    "    sns.histplot(data=dev_joined_with_audio, x=feature, hue='label', element='step', stat='density', common_norm=False, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {feature} by label')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Density')\n",
    "\n",
    "# Adjust layout to prevent overlap and save the figure if needed\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14f2a9d-7e41-4546-8f7f-a910be55e4f2",
   "metadata": {},
   "source": [
    "def extract_features(waveform, sr):\n",
    "    \"\"\"\n",
    "    Calculate various spectral, rhythmic, and tonal features and return them in a dictionary.\n",
    "    \"\"\"\n",
    "    # Decompose into harmonic and percussive components\n",
    "    harmonic, percussive = librosa.effects.hpss(waveform)\n",
    "\n",
    "    # Temporal and spectral features\n",
    "    tempo, _ = librosa.beat.beat_track(y=waveform, sr=sr)\n",
    "    onset_env = librosa.onset.onset_strength(y=waveform, sr=sr)\n",
    "    tempogram = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr)\n",
    "    fourier_tempogram = librosa.feature.fourier_tempogram(onset_envelope=onset_env, sr=sr)\n",
    "    tempogram_ratio = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr, win_length=16)\n",
    "    p_features = librosa.feature.poly_features(S=librosa.stft(waveform), sr=sr, order=2)\n",
    "    tonnetz = librosa.feature.tonnetz(y=harmonic, sr=sr)\n",
    "\n",
    "    # Basic spectral features\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=waveform, sr=sr)[0]\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=waveform, sr=sr)[0]\n",
    "    spectral_flatness = librosa.feature.spectral_flatness(y=waveform)[0]\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=waveform, sr=sr)[0]\n",
    "    rms_energy = librosa.feature.rms(y=waveform)[0]\n",
    "    zcr = librosa.feature.zero_crossing_rate(waveform)[0]\n",
    "    mfccs = librosa.feature.mfcc(y=waveform, sr=sr)\n",
    "    chroma = librosa.feature.chroma_stft(y=waveform, sr=sr)\n",
    "\n",
    "    # Initialize the feature dictionary\n",
    "    features = {\n",
    "        'centroid_mean': np.mean(spectral_centroid),\n",
    "        'bandwidth_mean': np.mean(spectral_bandwidth),\n",
    "        'flatness_mean': np.mean(spectral_flatness),\n",
    "        'rolloff_mean': np.mean(spectral_rolloff),\n",
    "        'rms_energy_mean': np.mean(rms_energy),\n",
    "        'zcr_mean': np.mean(zcr),\n",
    "        'tempo': tempo,\n",
    "        'tempogram_mean': np.mean(tempogram),\n",
    "        'fourier_tempogram_mean': np.mean(fourier_tempogram),\n",
    "        'tempogram_ratio_mean': np.mean(tempogram_ratio),\n",
    "        'tonnetz_mean': np.mean(tonnetz)\n",
    "    }\n",
    "\n",
    "    # Adding MFCCs and Chroma features\n",
    "    for i in range(mfccs.shape[0]):\n",
    "        features[f'mfccs_mean_{i}'] = np.mean(mfccs[i, :])\n",
    "\n",
    "    for i in range(chroma.shape[0]):\n",
    "        features[f'chroma_mean_{i}'] = np.mean(chroma[i, :])\n",
    "\n",
    "    # Adding Polynomial features\n",
    "    for i in range(p_features.shape[0]):\n",
    "        features[f'poly_features_{i}'] = np.mean(p_features[i, :])\n",
    "\n",
    "    return features\n",
    "\n",
    "def process_audio_data(audio_dict, sample_rate):\n",
    "    \"\"\"\n",
    "    Process each audio scene, extract features, and compile them into a single DataFrame.\n",
    "    \"\"\"\n",
    "    feature_list = []\n",
    "\n",
    "    for scene_id, data in audio_dict.items():\n",
    "        waveform = data['waveforms'][0].numpy()  # Ensure waveform is a NumPy array\n",
    "        features = extract_features(waveform, sample_rate)\n",
    "        features['scene_id'] = scene_id  # Add scene_id to the features dictionary\n",
    "        feature_list.append(features)\n",
    "    \n",
    "    # Create a DataFrame from the list of feature dictionaries\n",
    "    combined_features_df = pd.DataFrame(feature_list)\n",
    "    return combined_features_df\n",
    "\n",
    "sample_rate = 16000  # Common sample rate for high-quality audio\n",
    "# Example of usage in your pipeline\n",
    "train_feature_data = process_audio_data(train_audio_dict, sample_rate)\n",
    "print(train_feature_data.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d50c62-7da9-497a-8836-ff37292d301d",
   "metadata": {},
   "source": [
    "# Set the aesthetics for the plots\n",
    "sns.set(style='whitegrid')\n",
    "palette = {0: 'blue', 1: 'orange', 2: 'green'}\n",
    "\n",
    "# Enumerate over each feature in your list of audio features\n",
    "for i, feature in enumerate(audio_features):\n",
    "    # Create a new figure for each plot\n",
    "    plt.figure(figsize=(5, 3))  # You can adjust the figure size as necessary\n",
    "\n",
    "    # Plot each feature by label\n",
    "    sns.histplot(data=train_joined_with_audio, x=feature, hue='label', element='step', stat='density', common_norm=False, palette = palette)\n",
    "    \n",
    "    # Set the title and labels for each plot\n",
    "    plt.title(f'Distribution of {feature} by label')\n",
    "    plt.xlabel(f'{feature}')\n",
    "    plt.ylabel('Density')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703108f2-c9e9-4888-b873-417652926071",
   "metadata": {},
   "source": [
    "def positive_fft(waveform, sr):\n",
    "    \"\"\"\n",
    "    This function computes the FFT of a waveform and returns the positive frequency components and their magnitudes.\n",
    "    \n",
    "    Parameters:\n",
    "    - waveform: The audio waveform array.\n",
    "    - sr: Sample rate of the audio data.\n",
    "    \n",
    "    Returns:\n",
    "    - pos_frequencies: Positive frequency values.\n",
    "    - pos_magnitudes: Magnitudes of the FFT at positive frequencies.\n",
    "    \"\"\"\n",
    "    fft_values = np.fft.fft(waveform)\n",
    "    frequencies = np.fft.fftfreq(len(waveform), 1/sr)\n",
    "    \n",
    "    # Filter positive frequencies\n",
    "    mask = frequencies >= 0\n",
    "    pos_frequencies = frequencies[mask]\n",
    "    pos_magnitudes = np.abs(fft_values[mask])\n",
    "    \n",
    "    return pos_frequencies, pos_magnitudes"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacd21b-54b5-40e6-bd90-191dabcbbab3",
   "metadata": {},
   "source": [
    "def plot_average_fft(audio_dict, sample_rate):\n",
    "    sum_fft = None\n",
    "    count = 0\n",
    "\n",
    "    for scene_id, data in audio_dict.items():\n",
    "        waveform = data['waveforms'][0].numpy()\n",
    "        pos_frequencies, pos_magnitudes = positive_fft(waveform, sample_rate)\n",
    "        \n",
    "        # Initialize sum_fft if it's the first scene\n",
    "        if sum_fft is None:\n",
    "            sum_fft = np.zeros_like(pos_magnitudes)\n",
    "        \n",
    "        # Sum the FFT magnitudes\n",
    "        sum_fft += pos_magnitudes\n",
    "        count += 1\n",
    "\n",
    "    # Calculate average FFT\n",
    "    avg_fft = sum_fft / count\n",
    "\n",
    "    # Plot the average FFT\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(pos_frequencies, avg_fft)\n",
    "    plt.title('Average FFT Across All Scenes')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Average Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "plot_average_fft(dev_audio_dict, 44100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aacfdb8b-93ef-4e27-81cd-2eb19466cde8",
   "metadata": {},
   "source": [
    "def plot_filtered_maxima_by_label(audio_dict, sr, max_freq=5000):\n",
    "    \"\"\"\n",
    "    Computes and plots the maximum FFT magnitude values of waveforms in the audio dictionary, separated by labels, up to a specified maximum frequency.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_dict: Dictionary containing multiple audio data entries. Each entry is expected to have 'waveforms' key with an audio waveform array and 'label' key with the category.\n",
    "    - sr: Sample rate of the audio data.\n",
    "    - max_freq: The maximum frequency to consider for filtering.\n",
    "    \"\"\"\n",
    "    max_values = {0: [], 1: [], 2: []}\n",
    "\n",
    "    for scene_id, data in audio_dict.items():\n",
    "        label = data['label'].item()  # get the label\n",
    "        waveform = data['waveforms'][0].numpy()  # extract the first channel waveform and convert to numpy array if necessary\n",
    "        fft_values = np.fft.fft(waveform)\n",
    "        frequencies = np.fft.fftfreq(len(waveform), 1 / sr)\n",
    "        magnitudes = np.abs(fft_values)\n",
    "        \n",
    "        # Filter to only consider frequencies up to max_freq\n",
    "        mask = (frequencies >= 0) & (frequencies <= max_freq)\n",
    "        filtered_magnitudes = magnitudes[mask]\n",
    "\n",
    "        # Find the maximum magnitude within the filtered range\n",
    "        if len(filtered_magnitudes) > 0:\n",
    "            max_values[label].append(np.max(filtered_magnitudes))\n",
    "\n",
    "    # Plotting all labels on the same plot for max values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for label in max_values:\n",
    "        if max_values[label]:  # only plot if there are max values for this label\n",
    "            avg_max = np.mean(max_values[label])\n",
    "            plt.bar(f'Label {label}', avg_max, label=f'Label {label}')\n",
    "    \n",
    "    plt.title('Average Maximum FFT Magnitude Values by Label')\n",
    "    plt.xlabel('Sentiment Label')\n",
    "    plt.ylabel('Maximum FFT Magnitude')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_filtered_maxima_by_label(train_audio_dict, 44100, 2000)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e160219a-77e6-43ff-aa6a-b695edb5ff31",
   "metadata": {},
   "source": [
    "def plot_filtered_minima_by_label(audio_dict, sr, max_freq=5000):\n",
    "    \"\"\"\n",
    "    Computes and plots the minimum FFT magnitude values of waveforms in the audio dictionary, separated by labels, up to a specified maximum frequency.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_dict: Dictionary containing multiple audio data entries. Each entry is expected to have 'waveforms' key with an audio waveform array and 'label' key with the category.\n",
    "    - sr: Sample rate of the audio data.\n",
    "    - min_freq: The minimum frequency to consider for filtering.\n",
    "    \"\"\"\n",
    "    min_values = {0: [], 1: [], 2: []}\n",
    "\n",
    "    for scene_id, data in audio_dict.items():\n",
    "        label = data['label'].item()  # get the label\n",
    "        waveform = data['waveforms'][0].numpy()  # extract the first channel waveform and convert to numpy array if necessary\n",
    "        fft_values = np.fft.fft(waveform)\n",
    "        frequencies = np.fft.fftfreq(len(waveform), 1 / sr)\n",
    "        magnitudes = np.abs(fft_values)\n",
    "        \n",
    "        # Filter to only consider frequencies up to max_freq\n",
    "        mask = (frequencies >= 0) & (frequencies <= max_freq)\n",
    "        filtered_magnitudes = magnitudes[mask]\n",
    "\n",
    "        # Find the minimum magnitude within the filtered range\n",
    "        if len(filtered_magnitudes) > 0:\n",
    "            min_values[label].append(np.min(filtered_magnitudes))\n",
    "\n",
    "    # Plotting all labels on the same plot for max values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for label in min_values:\n",
    "        if min_values[label]:  # only plot if there are max values for this label\n",
    "            avg_min = np.mean(min_values[label])\n",
    "            plt.bar(f'Label {label}', avg_min, label=f'Label {label}')\n",
    "    \n",
    "    plt.title('Average Minimum FFT Magnitude Values by Label')\n",
    "    plt.xlabel('Sentiment Label')\n",
    "    plt.ylabel('Minimum FFT Magnitude')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_filtered_minima_by_label(train_audio_dict, 44100, 2000)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8447634d-bebc-4bf5-9209-3e726dc4a476",
   "metadata": {},
   "source": [
    "def plot_average_filtered_fft_by_label(audio_dict, sr, max_freq=5000):\n",
    "    \"\"\"\n",
    "    Computes and plots the average FFT of multiple waveforms stored in a dictionary, separated by labels, up to a specified maximum frequency.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_dict: Dictionary containing multiple audio data entries. Each entry is expected to have 'waveforms' key with an audio waveform array and 'label' key with the category.\n",
    "    - sr: Sample rate of the audio data.\n",
    "    - max_freq: The maximum frequency to display in the plot.\n",
    "    \"\"\"\n",
    "    sum_ffts = {0: None, 1: None, 2: None}\n",
    "    counts = {0: 0, 1: 0, 2: 0}\n",
    "\n",
    "    for scene_id, data in audio_dict.items():\n",
    "        label = data['label'].item()  # get the label\n",
    "        waveform = data['waveforms'][0].numpy()  # extract the first channel waveform and convert to numpy array if necessary\n",
    "        fft_values = np.fft.fft(waveform)\n",
    "        frequencies = np.fft.fftfreq(len(waveform), 1 / sr)\n",
    "        magnitudes = np.abs(fft_values)\n",
    "        \n",
    "        # Filter to only show up to max_freq\n",
    "        mask = (frequencies >= 0) & (frequencies <= max_freq)\n",
    "        filtered_frequencies = frequencies[mask]\n",
    "        filtered_magnitudes = magnitudes[mask]\n",
    "        \n",
    "        # Initialize sum_fft for the label if it's the first scene for that label\n",
    "        if sum_ffts[label] is None:\n",
    "            sum_ffts[label] = np.zeros_like(filtered_magnitudes)\n",
    "        \n",
    "        # Sum the FFT magnitudes for the label\n",
    "        sum_ffts[label] += filtered_magnitudes\n",
    "        counts[label] += 1\n",
    "\n",
    "    # Plotting all labels on the same plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for label in sum_ffts:\n",
    "        if counts[label] > 0:  # only plot if there are waveforms for this label\n",
    "            avg_fft = sum_ffts[label] / counts[label]\n",
    "            plt.plot(filtered_frequencies, avg_fft, label=f'Label {label}')\n",
    "    \n",
    "    plt.title('Average FFT Across Labels')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Average Amplitude')\n",
    "    plt.grid(True)\n",
    "    plt.ticks()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_average_filtered_fft_by_label(train_audio_dict, 44100, 2000)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fef6437-5e37-4f8a-95aa-fdaad98077df",
   "metadata": {},
   "source": [
    "def plot_average_spectrogram_by_label(audio_dict, sr, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Computes and plots the average spectrogram of multiple waveforms stored in a dictionary, separated by labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_dict: Dictionary containing audio data entries. Each entry is expected to have 'waveforms' key with an audio waveform array and 'label' key with the category.\n",
    "    - sr: Sample rate of the audio data.\n",
    "    - n_fft: Number of FFT components.\n",
    "    - hop_length: Number of samples between successive frames.\n",
    "    \"\"\"\n",
    "    sum_spectrograms = {0: None, 1: None, 2: None}\n",
    "    counts = {0: 0, 1: 0, 2: 0}\n",
    "\n",
    "    for scene_id, data in audio_dict.items():\n",
    "        label = data['label'].item()  # get the label\n",
    "        waveform = data['waveforms'][0].numpy()  # extract the first channel waveform and convert to numpy array if necessary\n",
    "        \n",
    "        # Compute the spectrogram\n",
    "        S = np.abs(librosa.stft(waveform, n_fft=n_fft, hop_length=hop_length))\n",
    "        \n",
    "        # Initialize sum_spectrogram for the label if it's the first scene for that label\n",
    "        if sum_spectrograms[label] is None:\n",
    "            sum_spectrograms[label] = np.zeros_like(S)\n",
    "        \n",
    "        # Sum the spectrogram for the label\n",
    "        sum_spectrograms[label] += S\n",
    "        counts[label] += 1\n",
    "\n",
    "    # Plotting all labels\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "    for label, ax in zip(sum_spectrograms, axes):\n",
    "        if counts[label] > 0:  # only plot if there are waveforms for this label\n",
    "            avg_spectrogram = sum_spectrograms[label] / counts[label]\n",
    "            img = librosa.display.specshow(librosa.amplitude_to_db(avg_spectrogram, ref=np.max),\n",
    "                                           sr=sr, hop_length=hop_length, x_axis='time', y_axis='log', ax=ax)\n",
    "            ax.set_title(f'Label {label}')\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_average_spectrogram_by_label(train_audio_dict, 44100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4f237a1-919a-4c36-987e-4db2145a0597",
   "metadata": {},
   "source": [
    "df = train_joined_with_audio\n",
    "\n",
    "# Select only the audio feature columns, excluding 'audio_feature_tempo'\n",
    "audio_features = [col for col in df.columns if col.startswith('audio_feature_')]\n",
    "\n",
    "# Prepare a list to store ANOVA results\n",
    "results = []\n",
    "\n",
    "# Perform ANOVA for each feature and store results in the list\n",
    "for feature in audio_features:\n",
    "    grouped_data = [df[df['label'] == label][feature] for label in df['label'].unique()]\n",
    "    fvalue, pvalue = stats.f_oneway(*grouped_data)\n",
    "    results.append({'Feature': feature.replace('audio_feature_', ''), 'F-Value': fvalue, 'P-Value': pvalue})\n",
    "\n",
    "# Convert list to DataFrame\n",
    "anova_results = pd.DataFrame(results)\n",
    "\n",
    "# Display the results sorted by p-value to see which features are most statistically significant\n",
    "anova_results_sorted = anova_results.sort_values(by='P-Value')\n",
    "print(anova_results_sorted)\n",
    "\n",
    "# Plot the results for a visual comparison\n",
    "plt.figure(figsize=(10, 8))  # Increased figure height for better label spacing\n",
    "plt.barh(anova_results_sorted['Feature'], anova_results_sorted['P-Value'], color='skyblue')\n",
    "plt.xlabel('P-Value')\n",
    "plt.title('P-Values for ANOVA Across Audio Features')\n",
    "plt.gca()  # Invert y axis to have the lowest p-values on top\n",
    "\n",
    "# Adjust margins and spacing\n",
    "plt.subplots_adjust(left=0.3)  # Adjust this value as needed to fit your feature names\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cae96108-3705-4d41-8274-15107604d472",
   "metadata": {},
   "source": [
    "df = train_joined_with_audio\n",
    "\n",
    "# Select only the audio feature columns, excluding 'audio_feature_tempo'\n",
    "audio_features = [col for col in df.columns if col.startswith('audio_feature_') and col != 'audio_feature_tempo' and col != 'audio_feature_mfccs_mean_3']\n",
    "\n",
    "# Prepare a list to store ANOVA results\n",
    "results = []\n",
    "\n",
    "# Perform ANOVA for each feature and store results in the list\n",
    "for feature in audio_features:\n",
    "    grouped_data = [df[df['label'] == label][feature] for label in df['label'].unique()]\n",
    "    fvalue, pvalue = stats.f_oneway(*grouped_data)\n",
    "    results.append({'Feature': feature.replace('audio_feature_', ''), 'F-Value': fvalue, 'P-Value': pvalue})\n",
    "\n",
    "# Convert list to DataFrame\n",
    "anova_results = pd.DataFrame(results)\n",
    "\n",
    "# Display the results sorted by p-value to see which features are most statistically significant\n",
    "anova_results_sorted = anova_results.sort_values(by='P-Value')\n",
    "print(anova_results_sorted)\n",
    "\n",
    "# Plot the results for a visual comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(anova_results_sorted['Feature'], anova_results_sorted['P-Value'], color='skyblue')\n",
    "plt.xlabel('P-Value')\n",
    "plt.title('P-Values for ANOVA Across Audio Features (excluding tempo)')\n",
    "plt.gca().invert_yaxis()  # Invert y axis to have the lowest p-values on top\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b8f80cb-527c-4456-be62-a5324fd9dd1a",
   "metadata": {},
   "source": [
    "def plot_average_filtered_fft(audio_dict, sr, max_freq=5000):\n",
    "    \"\"\"\n",
    "    Computes the average FFT of multiple waveforms stored in a dictionary, and plots the magnitudes up to a specified maximum frequency.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_dict: Dictionary containing multiple audio data entries. Each entry is expected to have a 'waveforms' key with an audio waveform array.\n",
    "    - sr: Sample rate of the audio data.\n",
    "    - max_freq: The maximum frequency to display in the plot.\n",
    "    \"\"\"\n",
    "    sum_fft = None\n",
    "    count = 0\n",
    "\n",
    "    for scene_id, data in audio_dict.items():\n",
    "        waveform = data['waveforms'][0].numpy()  # Extract waveform and convert to numpy array if necessary\n",
    "        fft_values = np.fft.fft(waveform)\n",
    "        frequencies = np.fft.fftfreq(len(waveform), 1 / sr)\n",
    "        magnitudes = np.abs(fft_values)\n",
    "        \n",
    "        # Filter to only show up to max_freq\n",
    "        mask = (frequencies >= 0) & (frequencies <= max_freq)\n",
    "        filtered_frequencies = frequencies[mask]\n",
    "        filtered_magnitudes = magnitudes[mask]\n",
    "        \n",
    "        # Initialize sum_fft if it's the first scene\n",
    "        if sum_fft is None:\n",
    "            sum_fft = np.zeros_like(filtered_magnitudes)\n",
    "        \n",
    "        # Sum the FFT magnitudes\n",
    "        sum_fft += filtered_magnitudes\n",
    "        count += 1\n",
    "\n",
    "    # Calculate average FFT\n",
    "    avg_fft = sum_fft / count\n",
    "\n",
    "    # Plot the average FFT\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(filtered_frequencies, avg_fft)\n",
    "    plt.title('Average FFT Across All Scenes (Filtered)')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Average Amplitude')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_average_filtered_fft(train_audio_dict, 44100, 5000)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec3b0a52-6c34-4fa7-9db9-6a1281dfa372",
   "metadata": {},
   "source": [
    "def plot_feature_distributions(feature_data):\n",
    "    \"\"\"\n",
    "    Plots histograms for each spectral feature in the feature DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_data: DataFrame containing the extracted audio features.\n",
    "    \"\"\"\n",
    "    # Prepare the figure layout\n",
    "    plt.figure(figsize=(20, 20))  # Adjust size as needed for clarity\n",
    "\n",
    "    # # Automatically fetch all feature names except 'scene_id' if it's part of the DataFrame\n",
    "    # features = [col for col in feature_data.columns if col not in ['scene_id']]\n",
    "\n",
    "    # Determine the number of rows and columns for the subplot grid\n",
    "    total_features = len(audio_features)\n",
    "    columns = 4  # Number of columns in the plot grid\n",
    "    rows = (total_features + columns - 1) // columns  # Calculate required number of rows\n",
    "\n",
    "    # Create subplots for each feature\n",
    "    for i, feature in enumerate(audio_features):\n",
    "        ax = plt.subplot(rows, columns, i + 1)\n",
    "        # Use a consistent number of bins and alpha transparency for clarity\n",
    "        plt.hist(feature_data[feature], bins=30, alpha=0.7, color='blue')\n",
    "        plt.title(feature.replace('_', ' ').capitalize())\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_distributions(train_joined_with_audio)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1263b5e0-be6b-47c5-b6a6-c9d71f2845df",
   "metadata": {},
   "source": [
    "with open('../AudioFeaturesExtraction/train_audio_df.pkl', 'rb') as f:\n",
    "     train_audio_data = pickle.load(f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6ce4267-241b-4cdb-821a-c044b5911b6b",
   "metadata": {},
   "source": [
    "def plot_accuracy_with_values(data, title):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # Plot the accuracy without audio features\n",
    "    plt.plot(data['Model Features'], data['Accuracy'], label='Accuracy without Audio', marker='o', color='blue')\n",
    "    for i, txt in enumerate(data['Accuracy']):\n",
    "        plt.annotate(f\"{txt:.4f}\", (data['Model Features'][i], data['Accuracy'][i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    # Plot the accuracy with audio features\n",
    "    plt.plot(data['Model Features'], data['Accuracy with Audio features'], label='Accuracy with Audio', marker='o', color='green')\n",
    "    for i, txt in enumerate(data['Accuracy with Audio features']):\n",
    "        plt.annotate(f\"{txt:.4f}\", (data['Model Features'][i], data['Accuracy with Audio features'][i]), textcoords=\"offset points\", xytext=(0,-15), ha='center')\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Model Features')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0.466,0.48)\n",
    "    plt.show()\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = '../ModelAccuracy.xlsx'\n",
    "sheets = ['RandomForest1', 'XGBoost1']\n",
    "\n",
    "# Read the specified sheets into dataframes\n",
    "df_random_forest, df_xgboost = [pd.read_excel(file_path, sheet_name=sheet) for sheet in sheets]\n",
    "\n",
    "# Plotting the data for RandomForest and XGBoost with values next to the dots\n",
    "plot_accuracy_with_values(df_random_forest, 'RandomForest models accuracy comparison')\n",
    "plot_accuracy_with_values(df_xgboost, 'XGBoost models accuracy comparison')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08745fa1-070a-4dcd-8015-334b45298c78",
   "metadata": {},
   "source": [
    "file_path = '../ModelAccuracy.xlsx'\n",
    "\n",
    "df_random_forest = pd.read_excel(file_path, sheet_name='RandomForest1')\n",
    "df_xgboost = pd.read_excel(file_path, sheet_name='XGBoost1')\n",
    "\n",
    "# Calculate specific improvements: Text + Audio vs Text, and Text + FE vs Text\n",
    "df_random_forest['Text + Audio Improvement %'] = ((df_random_forest.loc[df_random_forest['Model Features'] == 'Text', 'Accuracy with Audio features'].values[0] -\n",
    "                                                   df_random_forest.loc[df_random_forest['Model Features'] == 'Text', 'Accuracy'].values[0]) /\n",
    "                                                   df_random_forest.loc[df_random_forest['Model Features'] == 'Text', 'Accuracy'].values[0]) * 100\n",
    "\n",
    "df_random_forest['Text + FE Improvement %'] = ((df_random_forest.loc[df_random_forest['Model Features'] == 'Text + FE', 'Accuracy'].values[0] -\n",
    "                                                df_random_forest.loc[df_random_forest['Model Features'] == 'Text', 'Accuracy'].values[0]) /\n",
    "                                                df_random_forest.loc[df_random_forest['Model Features'] == 'Text', 'Accuracy'].values[0]) * 100\n",
    "\n",
    "df_xgboost['Text + Audio Improvement %'] = ((df_xgboost.loc[df_xgboost['Model Features'] == 'Text', 'Accuracy with Audio features'].values[0] -\n",
    "                                             df_xgboost.loc[df_xgboost['Model Features'] == 'Text', 'Accuracy'].values[0]) /\n",
    "                                             df_xgboost.loc[df_xgboost['Model Features'] == 'Text', 'Accuracy'].values[0]) * 100\n",
    "\n",
    "df_xgboost['Text + FE Improvement %'] = ((df_xgboost.loc[df_xgboost['Model Features'] == 'Text + FE', 'Accuracy'].values[0] -\n",
    "                                          df_xgboost.loc[df_xgboost['Model Features'] == 'Text', 'Accuracy'].values[0]) /\n",
    "                                          df_xgboost.loc[df_xgboost['Model Features'] == 'Text', 'Accuracy'].values[0]) * 100\n",
    "\n",
    "# Creating a dataframe for plotting\n",
    "improvement_comparison = pd.DataFrame({\n",
    "    'Model': ['RF - Audio', 'RF - FE', 'XGBoost - Audio', 'XGBoost - FE'],\n",
    "    'Improvement %': [\n",
    "        df_random_forest['Text + Audio Improvement %'].values[0],\n",
    "        df_random_forest['Text + FE Improvement %'].values[0],\n",
    "        df_xgboost['Text + Audio Improvement %'].values[0],\n",
    "        df_xgboost['Text + FE Improvement %'].values[0]\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(improvement_comparison['Model'], improvement_comparison['Improvement %'], color=['blue', 'orange', 'blue', 'orange'])\n",
    "plt.xlabel('Model and Improvement Type')\n",
    "plt.ylabel('Improvement %')\n",
    "plt.ylim(0,2.65)\n",
    "plt.title('Improvement Comparisons for Text + Audio and Text + FE')\n",
    "plt.axhline(0, color='gray', linewidth=0.8)  # Add a line at zero for reference\n",
    "\n",
    "# Annotate the percentage on the bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, f\"{yval:.2f}%\", va='bottom' if yval < 0 else 'top', ha='center')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d135258-2fd7-4389-b10d-f37e19647bc7",
   "metadata": {},
   "source": [
    "df_random_forest = pd.read_excel(file_path, sheet_name='RandomForest1')\n",
    "df_xgboost = pd.read_excel(file_path, sheet_name='XGBoost1')\n",
    "\n",
    "# Calculate the improvements from the root model to its enhancement with audio for each model feature\n",
    "df_random_forest['Text Improvement %'] = ((df_random_forest.loc[df_random_forest['Model Features'] == 'Text', 'Accuracy with Audio features'].values[0] -\n",
    "                                           df_random_forest.loc[df_random_forest['Model Features'] == 'Text', 'Accuracy'].values[0]) /\n",
    "                                           df_random_forest.loc[df_random_forest['Model Features'] == 'Text', 'Accuracy'].values[0]) * 100\n",
    "\n",
    "df_random_forest['FE Improvement %'] = ((df_random_forest.loc[df_random_forest['Model Features'] == 'FE', 'Accuracy with Audio features'].values[0] -\n",
    "                                         df_random_forest.loc[df_random_forest['Model Features'] == 'FE', 'Accuracy'].values[0]) /\n",
    "                                         df_random_forest.loc[df_random_forest['Model Features'] == 'FE', 'Accuracy'].values[0]) * 100\n",
    "\n",
    "df_random_forest['Text + FE Improvement %'] = ((df_random_forest.loc[df_random_forest['Model Features'] == 'Text + FE', 'Accuracy with Audio features'].values[0] -\n",
    "                                                df_random_forest.loc[df_random_forest['Model Features'] == 'Text + FE', 'Accuracy'].values[0]) /\n",
    "                                                df_random_forest.loc[df_random_forest['Model Features'] == 'Text + FE', 'Accuracy'].values[0]) * 100\n",
    "\n",
    "df_xgboost['Text Improvement %'] = ((df_xgboost.loc[df_xgboost['Model Features'] == 'Text', 'Accuracy with Audio features'].values[0] -\n",
    "                                     df_xgboost.loc[df_xgboost['Model Features'] == 'Text', 'Accuracy'].values[0]) /\n",
    "                                     df_xgboost.loc[df_xgboost['Model Features'] == 'Text', 'Accuracy'].values[0]) * 100\n",
    "\n",
    "df_xgboost['FE Improvement %'] = ((df_xgboost.loc[df_xgboost['Model Features'] == 'FE', 'Accuracy with Audio features'].values[0] -\n",
    "                                   df_xgboost.loc[df_xgboost['Model Features'] == 'FE', 'Accuracy'].values[0]) /\n",
    "                                   df_xgboost.loc[df_xgboost['Model Features'] == 'FE', 'Accuracy'].values[0]) * 100\n",
    "\n",
    "df_xgboost['Text + FE Improvement %'] = ((df_xgboost.loc[df_xgboost['Model Features'] == 'Text + FE', 'Accuracy with Audio features'].values[0] -\n",
    "                                          df_xgboost.loc[df_xgboost['Model Features'] == 'Text + FE', 'Accuracy'].values[0]) /\n",
    "                                          df_xgboost.loc[df_xgboost['Model Features'] == 'Text + FE', 'Accuracy'].values[0]) * 100\n",
    "\n",
    "# Creating a dataframe for plotting\n",
    "improvement_comparison = pd.DataFrame({\n",
    "    'Model Configuration': ['RF-Text', 'RF-FE', 'RF-Text+FE', 'XGB-Text', 'XGB-FE', 'XGB-Text+FE'],\n",
    "    'Improvement %': [\n",
    "        df_random_forest['Text Improvement %'].values[0],\n",
    "        df_random_forest['FE Improvement %'].values[0],\n",
    "        df_random_forest['Text + FE Improvement %'].values[0],\n",
    "        df_xgboost['Text Improvement %'].values[0],\n",
    "        df_xgboost['FE Improvement %'].values[0],\n",
    "        df_xgboost['Text + FE Improvement %'].values[0]\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['blue', 'green', 'red', 'blue', 'green', 'red']\n",
    "bars = plt.bar(improvement_comparison['Model Configuration'], improvement_comparison['Improvement %'], color=colors)\n",
    "plt.xlabel('Model Configuration')\n",
    "plt.ylabel('Improvement %')\n",
    "plt.ylim(-0.2,2.65)\n",
    "plt.title('Improvement of Audio Features over Base Models')\n",
    "plt.axhline(0, color='gray', linewidth=0.8)  # Add a line at zero for reference\n",
    "\n",
    "# Annotate the percentage on the bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, f\"{yval:.2f}%\", va='bottom' if yval < 0 else 'top', ha='center')\n",
    "\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
