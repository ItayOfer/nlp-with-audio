{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.105004Z",
     "start_time": "2024-04-02T10:14:29.101328Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a658ca5455bd9de5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.212474Z",
     "start_time": "2024-04-02T10:14:29.189724Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# read data\n",
    "train_df = pd.read_csv('MELD.Raw/train/train_sent_emo.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "65debc96efc7e119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.247347Z",
     "start_time": "2024-04-02T10:14:29.242444Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# set encoders\n",
    "emotion_encoder = LabelEncoder()\n",
    "sentiment_encoder = LabelEncoder()\n",
    "emotion_encoder.fit(train_df['Emotion'])\n",
    "sentiment_encoder.fit(train_df['Sentiment'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba1b63308504f9d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.295414Z",
     "start_time": "2024-04-02T10:14:29.281036Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "emotion_distribution = train_df['Emotion'].value_counts(normalize=True)\n",
    "emotion_distribution"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "762679a4e8210a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.340847Z",
     "start_time": "2024-04-02T10:14:29.336349Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "sentiment_distribution = train_df['Sentiment'].value_counts(normalize=True)\n",
    "sentiment_distribution"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "162b3db59f65d383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.604985Z",
     "start_time": "2024-04-02T10:14:29.594970Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "train_df['word_count'] = train_df['Utterance'].apply(lambda x: len(x.split()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4e0ade905c2562ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.647795Z",
     "start_time": "2024-04-02T10:14:29.641540Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "train_df.groupby('Emotion')['word_count'].mean()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "782965e28c9af569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.899503Z",
     "start_time": "2024-04-02T10:14:29.893991Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "top_10_speakers = train_df.groupby('Speaker', as_index=False)['Speaker'].size()\n",
    "top_10_speakers = top_10_speakers.sort_values(by='size', ascending=False)\n",
    "top_10_speakers = top_10_speakers['Speaker'].values[0:10]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6044a61163474c9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.940238Z",
     "start_time": "2024-04-02T10:14:29.937525Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "top_10_speakers"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8a35172cdb57305f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:29.978428Z",
     "start_time": "2024-04-02T10:14:29.974947Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "train_df_top_10 = train_df[train_df['Speaker'].isin(top_10_speakers)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "44c595c056d0a4c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:30.031495Z",
     "start_time": "2024-04-02T10:14:30.026603Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "train_df_top_10.groupby('Speaker',)['word_count'].mean()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e1b4f4241ef4a3c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:30.061783Z",
     "start_time": "2024-04-02T10:14:30.057090Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "# Function to generate and display word clouds\n",
    "def generate_word_clouds(df, column):\n",
    "    # Unique categories\n",
    "    categories = df[column].unique()\n",
    "    \n",
    "    # Figure setup\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Generate a word cloud for each category\n",
    "    for i, category in enumerate(categories, 1):\n",
    "        # Aggregate text for the current category\n",
    "        text = \" \".join(df[df[column] == category]['Utterance'].tolist())\n",
    "        \n",
    "        # Generate word cloud\n",
    "        wordcloud = WordCloud(width=400, height=300, background_color='white').generate(text)\n",
    "        \n",
    "        # Plot\n",
    "        plt.subplot(1, len(categories), i)\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'{category} {column}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5fb3987b3677f9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:31.487819Z",
     "start_time": "2024-04-02T10:14:30.091603Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "generate_word_clouds(train_df, 'Sentiment')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "165d411c5a3c55b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:32.651719Z",
     "start_time": "2024-04-02T10:14:31.488734Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "source": [
    "generate_word_clouds(train_df_top_10, 'Sentiment')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "755fbe02-1dc7-4c22-a800-5a91ebe092d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:34.968467Z",
     "start_time": "2024-04-02T10:14:32.652319Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "generate_word_clouds(train_df_top_10, 'Emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "55f2cb194d83cbb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:37.370298Z",
     "start_time": "2024-04-02T10:14:34.969591Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "generate_word_clouds(train_df, 'Emotion')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e309df33774f14e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:40.642564Z",
     "start_time": "2024-04-02T10:14:37.371041Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "generate_word_clouds(train_df_top_10, 'Speaker')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a63315cc-8562-417a-b796-a138d6e88ed7",
   "metadata": {},
   "source": [
    "**Audio Features Descriptive Statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca72f4d-db65-43d0-a163-817cb1757406",
   "metadata": {},
   "source": [
    "**First Version - simple audio features with 3 classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691be548-7592-4a15-981a-fc2c54cd0f96",
   "metadata": {},
   "source": [
    "#with open('../audio/dev_data.pkl','rb') as file:\n",
    "#    dev_audio_dict = pickle.load(file)\n",
    "\n",
    "#with open('../audio/test_data.pkl','rb') as file:\n",
    "#    test_audio_dict = pickle.load(file)\n",
    "\n",
    "with open('../AudioFeaturesExtraction/train_data.pkl','rb') as file:\n",
    "    train_audio_dict = pickle.load(file)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "258f9f43-14b8-4bd7-992e-b12d5d4ad941",
   "metadata": {},
   "source": [
    "def extract_features(waveform, sr):\n",
    "    \"\"\"\n",
    "    Calculate various spectral features and return them in a dictionary.\n",
    "    \"\"\"\n",
    "    # Basic spectral features\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=waveform, sr=sr)[0]\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=waveform, sr=sr)[0]\n",
    "    spectral_flatness = librosa.feature.spectral_flatness(y=waveform)[0]\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=waveform, sr=sr)[0]\n",
    "    rms_energy = librosa.feature.rms(y=waveform)[0]\n",
    "    zcr = librosa.feature.zero_crossing_rate(waveform)[0]\n",
    "    mfccs = librosa.feature.mfcc(y=waveform, sr=sr)\n",
    "    chroma = librosa.feature.chroma_stft(y=waveform, sr=sr)\n",
    "\n",
    "    # Initialize the feature dictionary\n",
    "    features = {\n",
    "        'centroid_mean': np.mean(spectral_centroid),\n",
    "        'bandwidth_mean': np.mean(spectral_bandwidth),\n",
    "        'flatness_mean': np.mean(spectral_flatness),\n",
    "        'rolloff_mean': np.mean(spectral_rolloff),\n",
    "        'rms_energy_mean': np.mean(rms_energy),\n",
    "        'zcr_mean': np.mean(zcr)\n",
    "    }\n",
    "\n",
    "    # Adding MFCCs and Chroma features\n",
    "    for i in range(mfccs.shape[0]):  # Assuming MFCCs are returned with shape (n_mfcc, t)\n",
    "        features[f'mfccs_mean_{i}'] = np.mean(mfccs[i, :])\n",
    "\n",
    "    for i in range(chroma.shape[0]):  # Assuming Chroma features are returned with shape (n_chroma, t)\n",
    "        features[f'chroma_mean_{i}'] = np.mean(chroma[i, :])\n",
    "\n",
    "    return features\n",
    "\n",
    "def process_audio_data(audio_dict, sample_rate):\n",
    "    \"\"\"\n",
    "    Process each audio scene, extract features, and compile them into a single DataFrame.\n",
    "    \"\"\"\n",
    "    feature_list = []\n",
    "\n",
    "    for scene_id, data in audio_dict.items():\n",
    "        waveform = data['waveforms'][0].numpy()  # Ensure waveform is a NumPy array\n",
    "        features = extract_features(waveform, sample_rate)\n",
    "        features['scene_id'] = scene_id  # Add scene_id to the features dictionary\n",
    "        feature_list.append(features)\n",
    "    \n",
    "    # Create a DataFrame from the list of feature dictionaries\n",
    "    combined_features_df = pd.DataFrame(feature_list)\n",
    "    return combined_features_df\n",
    "\n",
    "sample_rate = 16000  # Common sample rate for high-quality audio\n",
    "#dev_feature_data = process_audio_data(dev_audio_dict, sample_rate)\n",
    "#test_feature_data = process_audio_data(test_audio_dict, sample_rate)\n",
    "train_feature_data = process_audio_data(train_audio_dict, sample_rate)\n",
    "\n",
    "#print(dev_feature_data.head())\n",
    "#print(test_feature_data.head())\n",
    "print(train_feature_data.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5072d80-f7c8-4378-9ce6-9c61b0d09e83",
   "metadata": {},
   "source": [
    "dev = pd.read_csv('dev_fe_16000.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "744d57d2-5324-45cb-ac4d-517538cb506d",
   "metadata": {},
   "source": [
    "def file_key_generator(file_path, labels_dict):\n",
    "    info_file = pd.read_csv(file_path)\n",
    "    # Creating file_key which is a unique identifier for each scene.\n",
    "    info_file['file_key'] = 'dia' + info_file['Dialogue_ID'].astype(str) + '_' + 'utt' + info_file[\n",
    "        'Utterance_ID'].astype(str)\n",
    "\n",
    "    info_file['label'] = info_file['Sentiment'].map(labels_dict)\n",
    "    info_file = info_file.sort_values(by='file_key')\n",
    "    return info_file\n",
    "\n",
    "labels_dict = {\n",
    "            'negative': 0,\n",
    "            'neutral': 1,\n",
    "            'positive': 0}\n",
    "file_path = '/Users/orl/studies/statistical learning/nlp-with-audio/MELD.Raw/train/train_sent_emo.csv'\n",
    "\n",
    "dev_info_file = file_key_generator(file_path, labels_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b1065b7-d343-4af7-8129-7a7cdc5b01a3",
   "metadata": {},
   "source": [
    "dev_joined_with_audio = dev.merge(dev_info_file[['file_key', 'label']], left_on = 'scene_id', right_on = 'file_key')\n",
    "dev_joined_with_audio = dev_joined_with_audio.drop(columns=['Unnamed: 0'])\n",
    "dev_joined_with_audio = dev_joined_with_audio.drop(columns=['fourier_tempogram_mean', 'poly_features_0', 'poly_features_1', 'poly_features_2'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "def6b9d0-0cee-4df3-9d72-1cd8c0e9ff78",
   "metadata": {},
   "source": [
    "dev_joined_with_audio.columns"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07889695-f5ec-4029-95cc-efd23fc3a103",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "140416e2-77d9-440f-a7d9-a649932acd5f",
   "metadata": {},
   "source": [
    "num_cols = ['centroid_median', 'bandwidth_median', 'flatness_median',\n",
    "       'rolloff_median', 'rms_energy_median', 'zcr_median', 'tempo',\n",
    "       'tempogram_median', 'tempogram_ratio_median', 'tonnetz_median',\n",
    "       'mfccs_median_0', 'mfccs_median_1', 'mfccs_median_2', 'mfccs_median_3',\n",
    "       'mfccs_median_4', 'mfccs_median_5', 'mfccs_median_6', 'mfccs_median_7',\n",
    "       'mfccs_median_8', 'mfccs_median_9', 'mfccs_median_10',\n",
    "       'mfccs_median_11', 'mfccs_median_12', 'mfccs_median_13',\n",
    "       'mfccs_median_14', 'mfccs_median_15', 'mfccs_median_16',\n",
    "       'mfccs_median_17', 'mfccs_median_18', 'mfccs_median_19',\n",
    "       'chroma_median_0', 'chroma_median_1', 'chroma_median_2',\n",
    "       'chroma_median_3', 'chroma_median_4', 'chroma_median_5',\n",
    "       'chroma_median_6', 'chroma_median_7', 'chroma_median_8',\n",
    "       'chroma_median_9', 'chroma_median_10', 'chroma_median_11']\n",
    "df_cor = dev_joined_with_audio[num_cols].corr(method='spearman')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8f003-df9b-4759-9803-f0a0c7a39a8e",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1509c473-2d0e-4778-8689-76db589521a1",
   "metadata": {},
   "source": [
    "dev_joined_with_audio['label'].value_counts(normalize=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63341daa-4097-436b-815a-618204c5de18",
   "metadata": {},
   "source": [
    "dev_joined_with_audio['label'].value_counts(normalize=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f373aac-cf65-4b5b-bc57-7896dcc165b7",
   "metadata": {},
   "source": [
    "features = dev_joined_with_audio.columns[:-3]\n",
    "\n",
    "# Set the aesthetics for the plots\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Create a figure to hold the plots\n",
    "fig, axes = plt.subplots(10, 4, figsize=(20, 50))  # Adjust the layout size as necessary\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    # Plot each feature by label in the same subplot\n",
    "    sns.histplot(data=dev_joined_with_audio, x=feature, hue='label', element='step', stat='density', common_norm=False, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {feature} by label')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Density')\n",
    "\n",
    "# Adjust layout to prevent overlap and save the figure if needed\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7ed8758d-a701-4595-9b5d-0dc94fda5be2",
   "metadata": {},
   "source": [
    "**Second Version - simple audio features with 2 classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54589869-7cee-4e35-b7ed-6e66ff5dfcac",
   "metadata": {},
   "source": [
    "# Assuming 'df' is your DataFrame after joining and cleaning\n",
    "features = dev_joined_with_audio.columns[:-3]\n",
    "print(features)\n",
    "label = dev_joined_with_audio['label']\n",
    "\n",
    "log_likelihoods = []\n",
    "\n",
    "for feature in features:\n",
    "    # Prepare the feature data with an intercept\n",
    "    X = sm.add_constant(dev_joined_with_audio[feature])  # Adds a constant term to the feature\n",
    "    y = label\n",
    "\n",
    "    # Fit logistic regression model\n",
    "    model = sm.Logit(y, X).fit(disp=0)  # disp=0 turns off the fitting summary output\n",
    "\n",
    "    # Store the log-likelihood\n",
    "    log_likelihoods.append(model.llf)  # llf is the log likelihood of the fitted model\n",
    "\n",
    "# Create a DataFrame to sort features by log-likelihood\n",
    "results = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Log-Likelihood': log_likelihoods\n",
    "}).sort_values(by='Log-Likelihood', ascending=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'results' DataFrame from your code is already prepared and sorted\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Scatter plot where we use the index as the y-value and log-likelihood as the x-value\n",
    "plt.scatter(results['Log-Likelihood'], range(len(results['Feature'])), color='b')\n",
    "\n",
    "# Setting the y-ticks to show feature names\n",
    "plt.yticks(range(len(results['Feature'])), results['Feature'])\n",
    "\n",
    "plt.title('Log-Likelihood of Logistic Regression Models by Feature')\n",
    "plt.xlabel('Log-Likelihood')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6928590d-ee5a-44ff-829a-358eea5810a8",
   "metadata": {},
   "source": [
    "best_features = results.tail(12)['Feature'].values.tolist()\n",
    "best_features"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "74faef8a-bd26-4a0b-896d-9b8588557966",
   "metadata": {},
   "source": [
    "**Third Version - more audio features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14f2a9d-7e41-4546-8f7f-a910be55e4f2",
   "metadata": {},
   "source": [
    "def extract_features(waveform, sr):\n",
    "    \"\"\"\n",
    "    Calculate various spectral, rhythmic, and tonal features and return them in a dictionary.\n",
    "    \"\"\"\n",
    "    # Decompose into harmonic and percussive components\n",
    "    harmonic, percussive = librosa.effects.hpss(waveform)\n",
    "\n",
    "    # Temporal and spectral features\n",
    "    tempo, _ = librosa.beat.beat_track(y=waveform, sr=sr)\n",
    "    onset_env = librosa.onset.onset_strength(y=waveform, sr=sr)\n",
    "    tempogram = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr)\n",
    "    fourier_tempogram = librosa.feature.fourier_tempogram(onset_envelope=onset_env, sr=sr)\n",
    "    tempogram_ratio = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr, win_length=16)\n",
    "    p_features = librosa.feature.poly_features(S=librosa.stft(waveform), sr=sr, order=2)\n",
    "    tonnetz = librosa.feature.tonnetz(y=harmonic, sr=sr)\n",
    "\n",
    "    # Basic spectral features\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=waveform, sr=sr)[0]\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=waveform, sr=sr)[0]\n",
    "    spectral_flatness = librosa.feature.spectral_flatness(y=waveform)[0]\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=waveform, sr=sr)[0]\n",
    "    rms_energy = librosa.feature.rms(y=waveform)[0]\n",
    "    zcr = librosa.feature.zero_crossing_rate(waveform)[0]\n",
    "    mfccs = librosa.feature.mfcc(y=waveform, sr=sr)\n",
    "    chroma = librosa.feature.chroma_stft(y=waveform, sr=sr)\n",
    "\n",
    "    # Initialize the feature dictionary\n",
    "    features = {\n",
    "        'centroid_mean': np.mean(spectral_centroid),\n",
    "        'bandwidth_mean': np.mean(spectral_bandwidth),\n",
    "        'flatness_mean': np.mean(spectral_flatness),\n",
    "        'rolloff_mean': np.mean(spectral_rolloff),\n",
    "        'rms_energy_mean': np.mean(rms_energy),\n",
    "        'zcr_mean': np.mean(zcr),\n",
    "        'tempo': tempo,\n",
    "        'tempogram_mean': np.mean(tempogram),\n",
    "        'fourier_tempogram_mean': np.mean(fourier_tempogram),\n",
    "        'tempogram_ratio_mean': np.mean(tempogram_ratio),\n",
    "        'tonnetz_mean': np.mean(tonnetz)\n",
    "    }\n",
    "\n",
    "    # Adding MFCCs and Chroma features\n",
    "    for i in range(mfccs.shape[0]):\n",
    "        features[f'mfccs_mean_{i}'] = np.mean(mfccs[i, :])\n",
    "\n",
    "    for i in range(chroma.shape[0]):\n",
    "        features[f'chroma_mean_{i}'] = np.mean(chroma[i, :])\n",
    "\n",
    "    # Adding Polynomial features\n",
    "    for i in range(p_features.shape[0]):\n",
    "        features[f'poly_features_{i}'] = np.mean(p_features[i, :])\n",
    "\n",
    "    return features\n",
    "\n",
    "def process_audio_data(audio_dict, sample_rate):\n",
    "    \"\"\"\n",
    "    Process each audio scene, extract features, and compile them into a single DataFrame.\n",
    "    \"\"\"\n",
    "    feature_list = []\n",
    "\n",
    "    for scene_id, data in audio_dict.items():\n",
    "        waveform = data['waveforms'][0].numpy()  # Ensure waveform is a NumPy array\n",
    "        features = extract_features(waveform, sample_rate)\n",
    "        features['scene_id'] = scene_id  # Add scene_id to the features dictionary\n",
    "        feature_list.append(features)\n",
    "    \n",
    "    # Create a DataFrame from the list of feature dictionaries\n",
    "    combined_features_df = pd.DataFrame(feature_list)\n",
    "    return combined_features_df\n",
    "\n",
    "sample_rate = 16000  # Common sample rate for high-quality audio\n",
    "# Example of usage in your pipeline\n",
    "train_feature_data = process_audio_data(train_audio_dict, sample_rate)\n",
    "print(train_feature_data.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03a88dfc-fb4e-43b6-bcab-f1e7e91d7884",
   "metadata": {},
   "source": [
    "def file_key_generator(file_path, labels_dict):\n",
    "    info_file = pd.read_csv(file_path)\n",
    "    # Creating file_key which is a unique identifier for each scene.\n",
    "    info_file['file_key'] = 'dia' + info_file['Dialogue_ID'].astype(str) + '_' + 'utt' + info_file[\n",
    "        'Utterance_ID'].astype(str)\n",
    "\n",
    "    info_file['label'] = info_file['Sentiment'].map(labels_dict)\n",
    "    info_file = info_file.sort_values(by='file_key')\n",
    "    return info_file\n",
    "\n",
    "labels_dict = {\n",
    "            'negative': 0,\n",
    "            'neutral': 1,\n",
    "            'positive': 2}\n",
    "file_path = 'C:/Users/itay/PycharmProjects/nlp-with-audio/MELD.Raw/train/train_sent_emo.csv'\n",
    "\n",
    "train_info_file = file_key_generator(file_path, labels_dict)\n",
    "train_info_file_subset = train_info_file[['file_key','label']]\n",
    "print(train_info_file_subset.head())\n",
    "\n",
    "train_joined_with_audio = train_feature_data.merge(train_info_file_subset, left_on = 'scene_id', right_on = 'file_key')\n",
    "print(train_joined_with_audio.head())\n",
    "\n",
    "features = train_joined_with_audio.columns[:-3]\n",
    "\n",
    "# Set the aesthetics for the plots\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Create a figure to hold the plots\n",
    "fig, axes = plt.subplots(10, 5, figsize=(20, 50))  # Adjust the layout size as necessary\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    # Plot each feature by label in the same subplot\n",
    "    sns.histplot(data=train_joined_with_audio, x=feature, hue='label', element='step', stat='density', common_norm=False, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {feature} by label')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Density')\n",
    "\n",
    "# Adjust layout to prevent overlap and save the figure if needed\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01d50c62-7da9-497a-8836-ff37292d301d",
   "metadata": {},
   "source": [
    "def file_key_generator(file_path, labels_dict):\n",
    "    info_file = pd.read_csv(file_path)\n",
    "    # Creating file_key which is a unique identifier for each scene.\n",
    "    info_file['file_key'] = 'dia' + info_file['Dialogue_ID'].astype(str) + '_' + 'utt' + info_file[\n",
    "        'Utterance_ID'].astype(str)\n",
    "\n",
    "    info_file['label'] = info_file['Sentiment'].map(labels_dict)\n",
    "    info_file = info_file.sort_values(by='file_key')\n",
    "    return info_file\n",
    "\n",
    "labels_dict = {\n",
    "            'negative': 0,\n",
    "            'neutral': 1,\n",
    "            'positive': 0}\n",
    "file_path = 'C:/Users/itay/PycharmProjects/nlp-with-audio/MELD.Raw/train/train_sent_emo.csv'\n",
    "\n",
    "train_info_file = file_key_generator(file_path, labels_dict)\n",
    "train_info_file_subset = train_info_file[['file_key','label']]\n",
    "print(train_info_file_subset.head())\n",
    "\n",
    "train_joined_with_audio = train_feature_data.merge(train_info_file_subset, left_on = 'scene_id', right_on = 'file_key')\n",
    "print(train_joined_with_audio.head())\n",
    "\n",
    "features = train_joined_with_audio.columns[:-3]\n",
    "\n",
    "# Set the aesthetics for the plots\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Create a figure to hold the plots\n",
    "fig, axes = plt.subplots(10, 5, figsize=(20, 50))  # Adjust the layout size as necessary\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    # Plot each feature by label in the same subplot\n",
    "    sns.histplot(data=train_joined_with_audio, x=feature, hue='label', element='step', stat='density', common_norm=False, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {feature} by label')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Density')\n",
    "\n",
    "# Adjust layout to prevent overlap and save the figure if needed\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c08af861-5cc0-46cc-9ba8-643efe24678d",
   "metadata": {},
   "source": [
    "# Assuming 'df' is your DataFrame after joining and cleaning\n",
    "features = train_joined_with_audio.columns[:-3]\n",
    "label = train_joined_with_audio['label']\n",
    "\n",
    "log_likelihoods = []\n",
    "\n",
    "for feature in features:\n",
    "    # Prepare the feature data with an intercept\n",
    "    X = sm.add_constant(train_joined_with_audio[feature])  # Adds a constant term to the feature\n",
    "    y = label\n",
    "\n",
    "    # Fit logistic regression model\n",
    "    model = sm.Logit(y, X).fit(disp=0)  # disp=0 turns off the fitting summary output\n",
    "\n",
    "    # Store the log-likelihood\n",
    "    log_likelihoods.append(model.llf)  # llf is the log likelihood of the fitted model\n",
    "\n",
    "# Create a DataFrame to sort features by log-likelihood\n",
    "results = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Log-Likelihood': log_likelihoods\n",
    "}).sort_values(by='Log-Likelihood', ascending=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Log-Likelihood', y='Feature', data=results)\n",
    "plt.title('Log-Likelihood of Logistic Regression Models by Feature')\n",
    "plt.xlabel('Log-Likelihood')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703108f2-c9e9-4888-b873-417652926071",
   "metadata": {},
   "source": [
    "def positive_fft(waveform, sr):\n",
    "    \"\"\"\n",
    "    This function computes the FFT of a waveform and returns the positive frequency components and their magnitudes.\n",
    "    \n",
    "    Parameters:\n",
    "    - waveform: The audio waveform array.\n",
    "    - sr: Sample rate of the audio data.\n",
    "    \n",
    "    Returns:\n",
    "    - pos_frequencies: Positive frequency values.\n",
    "    - pos_magnitudes: Magnitudes of the FFT at positive frequencies.\n",
    "    \"\"\"\n",
    "    fft_values = np.fft.fft(waveform)\n",
    "    frequencies = np.fft.fftfreq(len(waveform), 1/sr)\n",
    "    \n",
    "    # Filter positive frequencies\n",
    "    mask = frequencies >= 0\n",
    "    pos_frequencies = frequencies[mask]\n",
    "    pos_magnitudes = np.abs(fft_values[mask])\n",
    "    \n",
    "    return pos_frequencies, pos_magnitudes"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacd21b-54b5-40e6-bd90-191dabcbbab3",
   "metadata": {},
   "source": [
    "def plot_average_fft(audio_dict, sample_rate):\n",
    "    sum_fft = None\n",
    "    count = 0\n",
    "\n",
    "    for scene_id, data in audio_dict.items():\n",
    "        waveform = data['waveforms'][0].numpy()\n",
    "        pos_frequencies, pos_magnitudes = positive_fft(waveform, sample_rate)\n",
    "        \n",
    "        # Initialize sum_fft if it's the first scene\n",
    "        if sum_fft is None:\n",
    "            sum_fft = np.zeros_like(pos_magnitudes)\n",
    "        \n",
    "        # Sum the FFT magnitudes\n",
    "        sum_fft += pos_magnitudes\n",
    "        count += 1\n",
    "\n",
    "    # Calculate average FFT\n",
    "    avg_fft = sum_fft / count\n",
    "\n",
    "    # Plot the average FFT\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(pos_frequencies, avg_fft)\n",
    "    plt.title('Average FFT Across All Scenes')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Average Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "plot_average_fft(dev_audio_dict, 44100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8f80cb-527c-4456-be62-a5324fd9dd1a",
   "metadata": {},
   "source": [
    "def plot_average_filtered_fft(audio_dict, sr, max_freq=5000):\n",
    "    \"\"\"\n",
    "    Computes the average FFT of multiple waveforms stored in a dictionary, and plots the magnitudes up to a specified maximum frequency.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_dict: Dictionary containing multiple audio data entries. Each entry is expected to have a 'waveforms' key with an audio waveform array.\n",
    "    - sr: Sample rate of the audio data.\n",
    "    - max_freq: The maximum frequency to display in the plot.\n",
    "    \"\"\"\n",
    "    sum_fft = None\n",
    "    count = 0\n",
    "\n",
    "    for scene_id, data in audio_dict.items():\n",
    "        waveform = data['waveforms'][0].numpy()  # Extract waveform and convert to numpy array if necessary\n",
    "        fft_values = np.fft.fft(waveform)\n",
    "        frequencies = np.fft.fftfreq(len(waveform), 1 / sr)\n",
    "        magnitudes = np.abs(fft_values)\n",
    "        \n",
    "        # Filter to only show up to max_freq\n",
    "        mask = (frequencies >= 0) & (frequencies <= max_freq)\n",
    "        filtered_frequencies = frequencies[mask]\n",
    "        filtered_magnitudes = magnitudes[mask]\n",
    "        \n",
    "        # Initialize sum_fft if it's the first scene\n",
    "        if sum_fft is None:\n",
    "            sum_fft = np.zeros_like(filtered_magnitudes)\n",
    "        \n",
    "        # Sum the FFT magnitudes\n",
    "        sum_fft += filtered_magnitudes\n",
    "        count += 1\n",
    "\n",
    "    # Calculate average FFT\n",
    "    avg_fft = sum_fft / count\n",
    "\n",
    "    # Plot the average FFT\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(filtered_frequencies, avg_fft)\n",
    "    plt.title('Average FFT Across All Scenes (Filtered)')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Average Amplitude')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_average_filtered_fft(dev_audio_dict, 44100, 5000)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b0a52-6c34-4fa7-9db9-6a1281dfa372",
   "metadata": {},
   "source": [
    "def plot_feature_distributions(feature_data):\n",
    "    \"\"\"\n",
    "    Plots histograms for each spectral feature in the feature DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_data: DataFrame containing the extracted audio features.\n",
    "    \"\"\"\n",
    "    # Prepare the figure layout\n",
    "    plt.figure(figsize=(20, 20))  # Adjust size as needed for clarity\n",
    "\n",
    "    # Automatically fetch all feature names except 'scene_id' if it's part of the DataFrame\n",
    "    features = [col for col in feature_data.columns if col not in ['scene_id']]\n",
    "\n",
    "    # Determine the number of rows and columns for the subplot grid\n",
    "    total_features = len(features)\n",
    "    columns = 4  # Number of columns in the plot grid\n",
    "    rows = (total_features + columns - 1) // columns  # Calculate required number of rows\n",
    "\n",
    "    # Create subplots for each feature\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = plt.subplot(rows, columns, i + 1)\n",
    "        # Use a consistent number of bins and alpha transparency for clarity\n",
    "        plt.hist(feature_data[feature], bins=30, alpha=0.7, color='blue')\n",
    "        plt.title(feature.replace('_', ' ').capitalize())\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_distributions(feature_data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae0c78ae-cd32-4649-8012-544b423acbe3",
   "metadata": {},
   "source": [
    "def plot_feature_over_scenes(feature_data, feature_name):\n",
    "    \"\"\"\n",
    "    Plot a specified feature for each scene over time or sequence.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(feature_data['scene_id'], feature_data[feature_name], marker='o', linestyle='-')\n",
    "    plt.title(f'Average {feature_name.replace(\"_\", \" \").capitalize()} Over Scenes')\n",
    "    plt.xlabel('Scene ID')\n",
    "    plt.ylabel(f'Average {feature_name.replace(\"_\", \" \").capitalize()}')\n",
    "    plt.grid(True)\n",
    "    plt.xticks([])  # Hides the x-axis labels\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "features_to_plot = [col for col in feature_data.columns if col != 'scene_id']\n",
    "\n",
    "for feature in features_to_plot:\n",
    "    plot_feature_over_scenes(feature_data, feature)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d0e38f-f9b3-4270-8521-9346639e2b91",
   "metadata": {},
   "source": [
    "def check_first_audio_properties(audio_dict):\n",
    "    \"\"\"\n",
    "    Check properties of the first waveform in the dictionary of audio data.\n",
    "\n",
    "    :param audio_dict: Dictionary containing 'waveforms' and 'labels' for audio data.\n",
    "    \"\"\"\n",
    "    # Get the first item from the dictionary\n",
    "    first_key = next(iter(audio_dict))\n",
    "    audio_data = audio_dict[first_key]\n",
    "    \n",
    "    waveform = audio_data['waveforms']\n",
    "    label = audio_data['label']\n",
    "    \n",
    "    # Assuming waveform is already a tensor, just check if it needs conversion from tensor\n",
    "    if hasattr(waveform, 'numpy'):  # Convert PyTorch tensor to numpy if necessary\n",
    "        waveform = waveform.numpy()\n",
    "\n",
    "    # Assume a single channel waveform or use the first channel\n",
    "    if waveform.ndim > 1:\n",
    "        waveform = waveform[0]\n",
    "\n",
    "    # Sample rate needs to be known beforehand since it's not stored in the dict\n",
    "    sample_rate = 16000   # Example fixed sample rate; replace with actual if known or if varied, handle accordingly\n",
    "\n",
    "    print(f\"Audio Path: {first_key}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Sample Rate: {sample_rate}\")\n",
    "    print(f\"Max Amplitude in Waveform: {np.max(np.abs(waveform))}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(waveform)\n",
    "    plt.title(f\"Waveform of {first_key}\")\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Load your train_data.pkl dictionary\n",
    "# with open('../audio/train_data.pkl', 'rb') as f:\n",
    "    # train_data_dict = pickle.load(f)\n",
    "\n",
    "check_first_audio_properties(train_data_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1263b5e0-be6b-47c5-b6a6-c9d71f2845df",
   "metadata": {},
   "source": [
    "with open('../AudioFeaturesExtraction/train_audio_df.pkl', 'rb') as f:\n",
    "     train_audio_data = pickle.load(f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce4267-241b-4cdb-821a-c044b5911b6b",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
